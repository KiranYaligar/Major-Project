# Requirements

## Non-functional

1. The proccessing of each spark query should be done in 2-3min.
2. The databrick(azure) notebook is cloud based therefore it is scalable.
3. The proposed system must be  able to handle large volume of data and process the data that keeps increasing  in an efficient manner to produce rich insights and analysis.

## Functional

1. The Databrick must be able to use all kind of dataset file i.e csv,text,parquet,json etc for running spark jobs.
2. The spark jobs shall be run on data in efficient file format(parquet file).